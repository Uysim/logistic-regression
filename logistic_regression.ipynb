{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic-regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uysim/logistic-regression/blob/master/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h9sUmPuD8nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GELVcoEPPx72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breast_cancer = load_breast_cancer()\n",
        "data = breast_cancer[\"data\"]\n",
        "target = breast_cancer[\"target\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGporKbKQAvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BevdJvcQQiQm",
        "colab_type": "text"
      },
      "source": [
        "Since the output can be rounded by sigmoid function to 0 or 1 because some input values are too small or too large, we later get the logarithm of zero which equals to the negative infinitely large number and cannot be used for further computations. That’s why we manually bound our sigmoid function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTiclXqQhHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return np.maximum(np.minimum(1 / (1 + np.exp(-x)), 0.9999), 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2if7RUvRMEk",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.imgur.com/Ke0U8oG.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2S_10dxV5DM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lost_function(x, y, theta):\n",
        "  t = x.dot(theta)\n",
        "  return -np.sum(y * np.log(sigmoid(t)) + (1 - y) * np.log(1 - sigmoid(t)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nI5lPMAWRED",
        "colab_type": "text"
      },
      "source": [
        "we need to define the cost function for the logistic regression. The most common approach is to iterate over training examples to apply sigmoid to them, then iterate one more time to count the sum of losses. \n",
        "\n",
        "The idea of cost function is that we count the sum of the metric distances between our hypothesis and real labels on the training data. The more optimized our parameters are, the less is the distance between the true value and hypothesis. But how can we minimize this distance?\n",
        "\n",
        "However, we use numpy to apply sigmoid to the whole array and count losses of all the array with just a few lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWA_3fh6V_-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost_function(x, y, theta):\n",
        "  return lost_function(x, y, theta) / x.shape[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbJf9oXmYbLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_cost_function(x, y, theta):\n",
        "  t = x.dot(theta)\n",
        "  return x.T.dot(y - sigmoid(t)) / x.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r-zYmMOYeRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_theta(x, y, theta, learning_rate):\n",
        "  return theta + learning_rate * gradient_cost_function(x, y, theta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFqT-ieRp599",
        "colab_type": "code",
        "outputId": "1f4bbaed-4870-47c3-fbb7-6a662278ed9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def test_train(x, y, learning_rate, iterations=500, threshold=0.0005):\n",
        "  theta = np.zeros(x.shape[1])\n",
        "  costs = []\n",
        "  print(theta)\n",
        "test_train(X_train, y_train, learning_rate=0.0001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyvkodvGSeWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(x, y, learning_rate, iterations=500, threshold=0.0005):\n",
        "  theta = np.zeros(x.shape[1])\n",
        "  costs = []\n",
        "  print(\"Start training\")\n",
        "  \n",
        "  for i in range(iterations):\n",
        "    theta = update_theta(x, y, theta, learning_rate)\n",
        "    cost = cost_function(x, y, theta)\n",
        "    print(f\"[Training step #{i}] — Cost function: {cost:.4f}\")\n",
        "    costs.append({\"cost\": cost, \"weights\": theta})\n",
        "             \n",
        "    if i > 15 and abs(costs[-2][\"cost\"] - costs[-1][\"cost\"]) < threshold:\n",
        "      break\n",
        "  return theta, costs\n",
        "\n",
        "theta, costs = train(X_train, y_train, learning_rate=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R473FGFCUlta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(x, theta):\n",
        "  return (sigmoid(x.dot(theta)) >= 0.5).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKhC_rNkUsM-",
        "colab_type": "text"
      },
      "source": [
        "Let’s compare, how predicted data are different than real:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylbEbvrJUyMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(x, y, theta):\n",
        "  y_pred = predict(x, theta)\n",
        "  return (y_pred == y).sum() / y.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaORmPb8Uy3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Accuracy on the training set: {get_accuracy(X_train, y_train, theta)}\")\n",
        "print(f\"Accuracy on the test set: {get_accuracy(X_test, y_test, theta)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFYzUosaXsT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.title(\"Model accuracy depending on the training step\")\n",
        "plt.plot(np.arange(0, len(costs)), [get_accuracy(X_train, y_train, c[\"weights\"]) for c in costs], alpha=0.7, label=\"Train\", color=\"r\")\n",
        "plt.plot(np.arange(0, len(costs)), [get_accuracy(X_test, y_test, c[\"weights\"]) for c in costs], alpha=0.7, label=\"Test\", color=\"b\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"Accuracy, %\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(True)\n",
        "plt.xticks(np.arange(0, len(costs)+1, 40))\n",
        "plt.yticks(np.arange(0.5, 1, 0.1))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paFX6lhGX_Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}